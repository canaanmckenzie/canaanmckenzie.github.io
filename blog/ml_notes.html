<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <title>ML notes</title>
    <meta name="author" content="Canaan McKenzie">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="/favicon.ico">
    <link rel="stylesheet" href="/style.css">
    <link type="application/atom+xml" rel="alternate" href="/feed.xml" title="Canaan McKenzie">
</head>
<body>
    <main>
        <header>Ongoing Notes in ML Algorithms and Theory</header>
            <article>
                <div class="title">
                    <h1 class="main-title">Simple ML for NLP</h1>
                    <p class="subtitle">Naive Bayes</p>
                </div>
                <p>An example algorithm for classification tasks. Relies on Bayes' theorem, calculating the probability of observing a class label given the set of features from a set of data. Assuming each feature is independent of all other features, e.g. counting domain-specific words (music-specific? painting-specific? etc). If we can correctly assume these words aren't correlated then we can classify a domain under a specific category. Simple to understand, fast to train and run.</p>
                <p>
                    <img src="./mlpics/bayestheorem.jpeg" alt="Bayes' Theorem">
                </p>
               
                <p class="subtitle"> Support Vector Machine</p>
                <p>An classification approach for training a "decision boundary" that acts to separate different categories of text (music vs painting to use the above example). Data points are separated by this boundary (linear or non linear although in practice isn't a linear boundary just a non-linear curve with linearity of 1? :) ). Class differences are correlated to how these data points relate to each other relative to the decision curve. The decision curve optimizes such that each data point from either data set is at its maximum. Will take time to train for large amounts of data.</p>
                <p><img src="./mlpics/svm.png" alt="SVM”"></p>
        
                <p class="subtitle">Hidden Markov Model</p>
                <p>Assuming unobservable process that generates data, try and model the hiddens states of the data that generate the observations. Markov assumption, assume each hidden state is dependent on previous state(s). Text data can be modeled assuming human language is sequential in nature (and current word is predicated on the word before...does this apply to grammar like german? [canaan]).</p>
     
                <p class="subtitle"> Conditional Random Fields</p>
                <p>Tags each word by classifying them to one of the parts of speech pool of all given POS tags. Takes sequence and context taggs and outperforms HMMs in POS tagging.</p>
            </article>

        <time datetime="2023-02-19T00:00:00+00:00">2023-02-19</time>

    </main>
    <footer>
        <a href="/">Index</a>
        <span>Copyright &copy; <a rel="author" href="/about">Canaan</a></span>
        <a href="/feed.xml">RSS</a>
    </footer>
    <a href="/sitemap.xml"></a>
</body>
</html>

