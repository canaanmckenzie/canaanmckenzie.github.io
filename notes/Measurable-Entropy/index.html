<!DOCTYPE html>
<html>
  <head>
<meta charset="utf-8">
<meta name = "viewport" content="width=device-width, initial-scaled=1.0"/>
<title>Measurable Entropy</title>
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="icon" href="/assets/images/favicon.ico">
<link type="application/atom+xml" rel="alternate" href="/feed.xml" />
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Measurable Entropy</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Measurable Entropy" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A brief introduction to basic information theory (entropy/information as a measure for theoretical unpredictability of data) and descriptive statistics (central tendency, dispersian, and shape). The maximilization of entropy under different constraints yields some common probability distributions: uniform distribution (given no prior knowledge); normal distribution (given that mean and variance are known)." />
<meta property="og:description" content="A brief introduction to basic information theory (entropy/information as a measure for theoretical unpredictability of data) and descriptive statistics (central tendency, dispersian, and shape). The maximilization of entropy under different constraints yields some common probability distributions: uniform distribution (given no prior knowledge); normal distribution (given that mean and variance are known)." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-03-25T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Measurable Entropy" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-03-25T00:00:00-04:00","datePublished":"2023-03-25T00:00:00-04:00","description":"A brief introduction to basic information theory (entropy/information as a measure for theoretical unpredictability of data) and descriptive statistics (central tendency, dispersian, and shape). The maximilization of entropy under different constraints yields some common probability distributions: uniform distribution (given no prior knowledge); normal distribution (given that mean and variance are known).","headline":"Measurable Entropy","mainEntityOfPage":{"@type":"WebPage","@id":"/notes/Measurable-Entropy/"},"url":"/notes/Measurable-Entropy/"}</script>
<!-- End Jekyll SEO tag -->


<script type="text/javascript" async src='https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML'></script>

</head>
  <body class="full-width">
    <header>
    <nav class= "group">
        <a href="/"><img class="badge" src="/assets/img/ghosty.png" alt="CM"></a>
        <a href="/">Home</a>
        <a href="/about">about</a>
    </nav>
</header>
    <article>
      <h1>Measurable Entropy</h1>
<p class="subtitle">March 25, 2023</p>


<p>A brief introduction to basic information theory (entropy/information as a measure for theoretical unpredictability of data) and descriptive statistics (central tendency, dispersian, and shape). The maximilization of entropy under different constraints yields some common probability distributions: uniform distribution (given no prior knowledge); normal distribution (given that mean and variance are known).</p>

<p>Shannon entropy:</p>
<div class="mathblock"><script type="math/tex; mode=display">
\begin{equation} 
H(X) = -\sum_{i=1}^{n} p(x_i) \log_2 p(x_i)
\end{equation}
</script></div>

<p>implemented here</p>
<div class="language-haskell highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kr">import</span> <span class="nn">Data.List</span> <span class="p">(</span><span class="nf">group</span><span class="p">,</span> <span class="nf">sort</span><span class="p">)</span>

<span class="n">shannonEntropy</span> <span class="o">::</span> <span class="p">[</span><span class="kt">Double</span><span class="p">]</span> <span class="o">-&gt;</span> <span class="kt">Double</span>
<span class="n">shannonEntropy</span> <span class="n">ps</span> <span class="o">=</span> <span class="n">negate</span> <span class="o">$</span> <span class="n">sum</span> <span class="o">$</span> <span class="n">map</span> <span class="p">(</span><span class="nf">\</span><span class="n">p</span> <span class="o">-&gt;</span> <span class="n">p</span> <span class="o">*</span> <span class="n">logBase</span> <span class="mi">2</span> <span class="n">p</span><span class="p">)</span> <span class="n">ps</span>
  <span class="kr">where</span> <span class="n">ps'</span> <span class="o">=</span> <span class="n">map</span> <span class="p">(</span><span class="nf">\</span><span class="n">g</span> <span class="o">-&gt;</span> <span class="n">fromIntegral</span> <span class="p">(</span><span class="n">length</span> <span class="n">g</span><span class="p">)</span> <span class="o">/</span> <span class="n">n</span><span class="p">)</span> <span class="n">grouped</span>
        <span class="n">grouped</span> <span class="o">=</span> <span class="n">group</span> <span class="o">$</span> <span class="n">sort</span> <span class="n">ps</span>
        <span class="n">n</span> <span class="o">=</span> <span class="n">fromIntegral</span> <span class="o">$</span> <span class="n">length</span> <span class="n">ps</span>

<span class="n">main</span> <span class="o">::</span> <span class="kt">IO</span> <span class="nb">()</span>
<span class="n">main</span> <span class="o">=</span> <span class="kr">do</span>
  <span class="kr">let</span> <span class="n">p</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.2</span><span class="p">,</span> <span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.4</span><span class="p">]</span>
  <span class="n">putStrLn</span> <span class="o">$</span> <span class="s">"Entropy of "</span> <span class="o">++</span> <span class="n">show</span> <span class="n">p</span> <span class="o">++</span> <span class="s">" is "</span> <span class="o">++</span> <span class="n">show</span> <span class="p">(</span><span class="n">shannonEntropy</span> <span class="n">p</span><span class="p">)</span>
</code></pre></div></div>

<p>For discrete random variables \(X\) with pmf <em>p(x)</em>:</p>

    </article>
    <div class="footer">
    <span>&2023 Canaan McKenzie. <a href="/feed.xml">Subscribe</a> via RSS</span>
</div>
  </body>
</html>