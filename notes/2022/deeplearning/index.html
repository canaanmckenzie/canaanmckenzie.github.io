<!DOCTYPE html>
<html lang="en">
	<head>
		<title>Deep Learning</title>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1">
		<link rel="stylesheet" type="text/css" href="./../../note-style.css">
		<link rel="icon" type="image/png" href="./../../../favicon.png">
	</head>
	<body>
		<header id="header" class="obviously-a-link">
			<nav><a href="../../../index.html">Home</a> ∕ <a href=
		"../../index.html">Notes</a> ∕</nav>
		<div><a href="./../../atom.xml">RSS</a>│<a href=
	"https://github.com/canaanmckenzie/canaanmckenzie.github.io">Source</a></div>
</header>
<main id="body">
<article>
	<div class="title">
		<h1 class="main-title">Simple DL for NLP</h1>
		<p class="subtitle">Recurrent Neural Networks</p>
	</div>
	<p>Assuming language is inherently sequential (a good start with human language), progressive reading of a text from one end to another is useful for understanding. RNNs keep sequential units to "remember" what has already been processed. Information is stored and updated at each tick for each word added into the network. Simple X(t) => [Unit Memory at t U(t)] => H(t). Useful for text classification, name entity recognition, translation, prediction of following text given preceeding inputs.</p>
	<div class="obviously-a-link">
		<a class="more-link" href="recurrentnn/rnn.html"> more..
		</a>
	</div>
	<p class="subtitle">Long Short-Term Memory Networks</p>
	<p>To circumvent RNNs general short term memory in the case of extended contextual inputs. LSTMNs (a subset of RNNs) rely on removal of context non-essential to solving of the given task. Instead of keeping the context in a singular vector, LSTMSNs rely on sub-vector circuits to computer H(t+1).</p>
	<div class="obviously-a-link">
		<a class="more-link" href="longshortterm/lstm.html"> more..
		</a>
	</div>
	<p class="subtitle">Convolutional Neural Networks</p>
	<p>Simplistically, replacing each word in a sentence with a word vector ( all of same size d) and stacking to form a matrix (2D in this case dimensions n x d) with n being the number of words in a sentence or phrase unit. Modelling n x d as an image.  Advantageous because the group of words is relative to the context.</p>
	<div class="obviously-a-link">
		<a class="more-link" href="convolutionalnn/cnn.html"> more..
		</a>
	</div>
	<p class="subtitle">Transformer Networks</p>
	<p>Modelling of text in a non-sequential manner, given a word or other input it at input adjacent to it. Transformers generate a representation of input with respect to the context, e.g. the word "sign" in context of a road could mean a physical sign, where in the context of a "discussion" it could mean sign language or a signal to another person. Transfer learning applies solutions to one representation is then applied to a related problem within a different context. by pre-training a large model with unsupervised learning to find nuanced patterns. Bidirectional Encoder Representations from Transformers from Google is an example of an open sourced model. Generally requires massive data sets to build the larger models</p>
	<div class="obviously-a-link">
		<a class="more-link" href="transformers/transformers.html"> more..
		</a>
	</div>
</div>
	<p class="subtitle">Autoencoder Networks</p>
	<p>Network designed for learning compressed vector representations of a given input. Generate a mapping function from input text to a vector, the reconstruct the input from the vector. Using the vector representations we generate as encodings as a representation for deciphering downstream inputs. Essentially compresses the data into a minimal representation of itself. (I tend to think of this a little like a one-time pad in old spycraft, but that's a semi-unreliable mental model [canaan]).</p>
	<div class="obviously-a-link">
		<a class="more-link" href="transformers/transformers.html"> more..
		</a>
</div>
</article>
</main>
</body>
</html>
