<!DOCTYPE html>
<html>
  <head>
<meta charset="utf-8">
<meta name = "viewport" content="width=device-width, initial-scaled=1.0"/>
<title>Measurable Entropy</title>
<link rel="stylesheet" href="/assets/css/style.css">
<link rel="icon" href="/assets/images/favicon.ico">
<link type="application/atom+xml" rel="alternate" href="/feed.xml" />
<!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Measurable Entropy</title>
<meta name="generator" content="Jekyll v4.3.2" />
<meta property="og:title" content="Measurable Entropy" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="A brief introduction to basic information theory (entropy/information as a measure for theoretical unpredictability of data) and descriptive statistics (central tendency, dispersian, and shape). The maximilization of entropy under different constraints yields some common probability distributions: uniform distribution (given no prior knowledge); normal distribution (given that mean and variance are known)." />
<meta property="og:description" content="A brief introduction to basic information theory (entropy/information as a measure for theoretical unpredictability of data) and descriptive statistics (central tendency, dispersian, and shape). The maximilization of entropy under different constraints yields some common probability distributions: uniform distribution (given no prior knowledge); normal distribution (given that mean and variance are known)." />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2023-03-25T00:00:00-04:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Measurable Entropy" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2023-03-25T00:00:00-04:00","datePublished":"2023-03-25T00:00:00-04:00","description":"A brief introduction to basic information theory (entropy/information as a measure for theoretical unpredictability of data) and descriptive statistics (central tendency, dispersian, and shape). The maximilization of entropy under different constraints yields some common probability distributions: uniform distribution (given no prior knowledge); normal distribution (given that mean and variance are known).","headline":"Measurable Entropy","mainEntityOfPage":{"@type":"WebPage","@id":"/articles/Measurable-Entropy/"},"url":"/articles/Measurable-Entropy/"}</script>
<!-- End Jekyll SEO tag -->


</head>
  <body class="full-width">
    <header>
    <nav class= "group">
        <a href="/"><img class="badge" src="/assets/img/ghosty.png" alt="CM"></a>
        <a href="/">Home</a>
        <a href="/about">about</a>
    </nav>
</header>
    <article>
      <h1>Measurable Entropy</h1>
<p class="subtitle">March 25, 2023</p>


<p>A brief introduction to basic information theory (entropy/information as a measure for theoretical unpredictability of data) and descriptive statistics (central tendency, dispersian, and shape). The maximilization of entropy under different constraints yields some common probability distributions: uniform distribution (given no prior knowledge); normal distribution (given that mean and variance are known).</p>

    </article>
    <div class="footer">
    <span>&copy 2014-2023 Canaan McKenzie. <a href="/feed.xml">Subscribe</a> via RSS</span>
</div>
  </body>
</html>